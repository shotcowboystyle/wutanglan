---
apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: loki
  namespace: monitoring

spec:
  interval: 5m

  chart:
    spec:
      # renovate: registryUrl=https://grafana.github.io/helm-charts
      chart: loki
      version: 2.13.2
      sourceRef:
        kind: HelmRepository
        name: grafana
        namespace: flux-system
      interval: 5m

  values:
    image:
      repository: ghcr.io/k8s-at-home/loki

    ingress:
      enabled: true

      ingressClassName: "traefik"
      annotations:
        cert-manager.io/cluster-issuer: "letsencrypt-staging"

      hosts:
        - host: &host "loki.${SECRET_DOMAIN}"
          paths:
            - /

      tls:
        - hosts:
            - *host

    serviceMonitor:
      enabled: true

    config:
      storage_config:
        # aws:
        #   s3: "s3://${LOKI_MINIO_ACCESS_KEY}:${LOKI_MINIO_SECRET_KEY}@${NAS_ADDR}:9000/loki"
        #   s3forcepathstyle: true
        aws:
          bucketnames: loki
          endpoint: "minio.dbms.svc.cluster.local:9000"
          access_key_id: ${SECRET_MINIO_LOKI_ACCESS_KEY}
          secret_access_key: ${SECRET_MINIO_LOKI_SECRET_KEY}
          region: ${SECRET_MINIO_REGION}
          s3forcepathstyle: true
          insecure: true

        boltdb_shipper:
          active_index_directory: /data/loki/index
          cache_location: /data/loki/index_cache
          resync_interval: 5s
          shared_store: s3

      ruler:
        storage:
          type: local
          local:
            directory: /rules

        rule_path: /tmp/scratch

        alertmanager_url: http://prometheus-alertmanager:9093

        ring:
          kvstore:
            store: inmemory

        enable_api: true

    alerting_groups:
      #
      # SMART Failures
      #
      - name: smart-failure
        rules:
          - alert: SmartFailures
            expr: |
              sum by (hostname) (count_over_time({hostname=~".+"} |~ "(?i).*smartd.*(error|fail).*"[2m])) > 0
            for: 10s
            labels:
              severity: critical
              category: logs
            annotations:
              # TODO: Figure out how to use hostname in summary
              summary: "SMART has reported failures"

      #
      # frigate
      #
      - name: frigate
        rules:
          - alert: FrigateUnableToReachMQTT
            expr: |
              sum(count_over_time({app="frigate"} |= "Unable to connect to MQTT server"[2m])) > 0
            for: 10s
            labels:
              severity: critical
              category: logs
            annotations:
              summary: "Frigate is unable to reach MQTT"

      #
      # *arr applications
      #
      - name: arr
        rules:
          - alert: ArrDatabaseIsLocked
            expr: |
              sum by (app) (count_over_time({app=~".*arr"} |= "database is locked"[2m])) > 0
            for: 10s
            labels:
              severity: critical
              category: logs
            annotations:
              summary: "{{$value}} is experiencing locked database issues"

      #
      # home-assistant
      #
      - name: home-assistant
        rules:
          - alert: HomeAssistantUnableToReachPostgresql
            expr: |
              sum by (app) (count_over_time({app=~"home-assistant"} |= "Error in database connectivity during commit"[2m])) > 0
            for: 10s
            labels:
              severity: critical
              category: logs
            annotations:
              summary: "Home Assistant is unable to connect to postgresql"

      #
      # vaultwarden
      #
      # - name: vaultwarden
      #   rules:
      #     - alert: VaultwardenUnableToReachPostgresql
      #       expr: |
      #         sum by (app) (count_over_time({app=~"vaultwarden"} |= "could not connect to server"[2m])) > 0
      #       for: 10s
      #       labels:
      #         severity: critical
      #         category: logs
      #       annotations:
      #         summary: "Vaultwarden is unable to connect to postgresql"
